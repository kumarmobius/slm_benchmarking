name: Build Gemma3LM v2.2
inputs:
  - { name: tokenizer_json, type: Model, description: "Tokenizer JSON file" }
  - { name: model_pt, type: Model, description: "Torch model .pt weights file" }
  - { name: model_config, type: Data, description: "Model config file (json)" }

outputs:
  - { name: gemma_lm, type: Model, description: "Serialized Gemma3LM object" }

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import torch
        import cloudpickle
        import psutil
        from tokenizers import Tokenizer
        from nesy_factory.language_model.gemma_wrapper import Gemma3LM
        
        parser = argparse.ArgumentParser()
        parser.add_argument("--tokenizer_json", required=True)
        parser.add_argument("--model_pt", required=True)
        parser.add_argument("--model_config", required=True)
        parser.add_argument("--gemma_lm", required=True)
        args = parser.parse_args()
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"[INFO] Device selected: {device}")
        
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            print(f"[INFO] GPUs available: {gpu_count}")
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                print(f"[INFO] GPU {i}: {gpu_name} | Memory: {gpu_memory:.2f} GB")
        else:
            print("[INFO] No GPU available, using CPU")
        
        ram = psutil.virtual_memory()
        ram_total = ram.total / (1024**3)
        ram_available = ram.available / (1024**3)
        print(f"[INFO] System RAM: {ram_total:.2f} GB total | {ram_available:.2f} GB available")
        
        import torch.nn as nn
        import torch.nn.functional as F
        
        def compute_rope_params(head_dim, theta_base=10_000.0, context_length=4096, dtype=torch.float32):
            assert head_dim % 2 == 0
            inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype) / head_dim))
            positions = torch.arange(context_length, dtype=dtype)
            angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)
            angles = torch.cat((angles, angles), dim=1)
            return torch.cos(angles), torch.sin(angles)
        
        def apply_rope(x, cos, sin):
            *_, seq_len, head_dim = x.size()
            x1, x2 = x[..., :head_dim//2], x[..., head_dim//2:]
            cos_seq = cos[:seq_len, :].to(x.dtype).unsqueeze(0).unsqueeze(0)
            sin_seq = sin[:seq_len, :].to(x.dtype).unsqueeze(0).unsqueeze(0)
            rotated = torch.cat((-x2, x1), dim=-1)
            return (x * cos_seq) + (rotated * sin_seq)
        
        class RMSNorm(nn.Module):
            def __init__(self, emb_dim, eps=1e-6, bias=False):
                super().__init__()
                self.eps = eps
                self.scale = nn.Parameter(torch.zeros(emb_dim))
                self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None
            
            def forward(self, x):
                orig_dtype = x.dtype
                x_f = x.float()
                var = x_f.pow(2).mean(dim=-1, keepdim=True)
                x_norm = x_f * torch.rsqrt(var + self.eps)
                out = x_norm * (1.0 + self.scale.float())
                if self.shift is not None:
                    out = out + self.shift.float()
                return out.to(orig_dtype)
        
        class GroupedQueryAttention(nn.Module):
            def __init__(self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, query_pre_attn_scalar=None, dtype=None):
                super().__init__()
                assert num_heads % num_kv_groups == 0
                self.num_heads = num_heads
                self.num_kv_groups = num_kv_groups
                self.group_size = num_heads // num_kv_groups
                self.head_dim = head_dim if head_dim is not None else d_in // num_heads
                self.scaling = (query_pre_attn_scalar ** -0.5) if (query_pre_attn_scalar is not None) else (self.head_dim ** -0.5)
                self.W_query = nn.Linear(d_in, num_heads * self.head_dim, bias=False, dtype=dtype)
                self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)
                self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)
                self.out_proj = nn.Linear(num_heads * self.head_dim, d_in, bias=False, dtype=dtype)
                self.q_norm = RMSNorm(self.head_dim) if qk_norm else None
                self.k_norm = RMSNorm(self.head_dim) if qk_norm else None
            
            def forward(self, x, cos, sin, mask=None):
                B, T, _ = x.shape
                q = self.W_query(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
                k = self.W_key(x).view(B, T, self.num_kv_groups, self.head_dim).transpose(1, 2)
                v = self.W_value(x).view(B, T, self.num_kv_groups, self.head_dim).transpose(1, 2)
                if self.q_norm is not None:
                    q = self.q_norm(q)
                if self.k_norm is not None:
                    k = self.k_norm(k)
                q = apply_rope(q, cos, sin)
                k = apply_rope(k, cos, sin)
                k = k.repeat_interleave(self.group_size, dim=1)
                v = v.repeat_interleave(self.group_size, dim=1)
                attn = torch.matmul(q, k.transpose(-2, -1)) * self.scaling
                if mask is not None:
                    attn = attn.masked_fill(~mask.unsqueeze(0).unsqueeze(0), float('-inf'))
                p = torch.softmax(attn, dim=-1)
                out = torch.matmul(p, v).transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)
                return self.out_proj(out)
        
        class FeedForward(nn.Module):
            def __init__(self, cfg):
                super().__init__()
                emb_dim = cfg["emb_dim"]
                hidden_dim = cfg["hidden_dim"]
                dtype = cfg["dtype"]
                self.fc1 = nn.Linear(emb_dim, hidden_dim, bias=False, dtype=dtype)
                self.fc2 = nn.Linear(emb_dim, hidden_dim, bias=False, dtype=dtype)
                self.fc3 = nn.Linear(hidden_dim, emb_dim, bias=False, dtype=dtype)
            
            def forward(self, x):
                return self.fc3(F.gelu(self.fc1(x)) * self.fc2(x))
        
        class TransformerBlock(nn.Module):
            def __init__(self, cfg, layer_type):
                super().__init__()
                self.layer_type = layer_type
                self.attn = GroupedQueryAttention(
                    d_in=cfg["emb_dim"],
                    num_heads=cfg["n_heads"],
                    num_kv_groups=cfg.get("n_kv_groups", 1),
                    head_dim=cfg["head_dim"],
                    qk_norm=cfg.get("qk_norm", False),
                    query_pre_attn_scalar=cfg.get("query_pre_attn_scalar", None),
                    dtype=cfg["dtype"]
                )
                self.input_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.post_attn_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.pre_ff_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.post_ff_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.ffn = FeedForward(cfg)
            
            def forward(self, x, cos, sin, mask=None):
                residual = x
                x = self.input_norm(x)
                x = self.post_attn_norm(self.attn(x, cos, sin, mask) + residual)
                residual = x
                x = self.pre_ff_norm(x)
                x = self.post_ff_norm(self.ffn(x) + residual)
                return x
        
        class Gemma3Model(nn.Module):
            def __init__(self, cfg):
                super().__init__()
                self.cfg = cfg
                vocab_size = cfg["vocab_size"]
                emb_dim = cfg["emb_dim"]
                self.token_emb = nn.Embedding(vocab_size, emb_dim, dtype=cfg["dtype"])
                self.blocks = nn.ModuleList([
                    TransformerBlock(cfg, layer_type=cfg["layer_types"][i])
                    for i in range(cfg["n_layers"])
                ])
                self.final_norm = RMSNorm(emb_dim, eps=cfg.get("rms_norm_eps", 1e-6))
                self.out_head = nn.Linear(emb_dim, vocab_size, bias=False, dtype=cfg["dtype"])
                self.cos_global, self.sin_global = compute_rope_params(
                    cfg["head_dim"], cfg["rope_base"], cfg["context_length"], torch.float32
                )
                self.cos_local, self.sin_local = compute_rope_params(
                    cfg["head_dim"], cfg["rope_local_base"], cfg["sliding_window"], torch.float32
                )
            
            def _ensure_rope_on_device(self, device):
                if self.cos_global.device != device:
                    self.cos_global = self.cos_global.to(device)
                    self.sin_global = self.sin_global.to(device)
                if self.cos_local.device != device:
                    self.cos_local = self.cos_local.to(device)
                    self.sin_local = self.sin_local.to(device)
            
            def forward(self, input_ids, labels=None):
                x = self.token_emb(input_ids).to(self.cfg["dtype"])
                B, T, _ = x.size()
                device = x.device
                self._ensure_rope_on_device(device)
                
                mask_full = torch.tril(torch.ones((T, T), dtype=torch.bool, device=device))
                window = self.cfg["sliding_window"]
                mask_sliding = torch.zeros((T, T), dtype=torch.bool, device=device)
                for i in range(T):
                    start = max(0, i - window + 1)
                    mask_sliding[i, start:i+1] = True
                
                for i, block in enumerate(self.blocks):
                    if self.cfg["layer_types"][i] == "sliding_attention":
                        x = block(x, self.cos_local, self.sin_local, mask_sliding)
                    else:
                        x = block(x, self.cos_global, self.sin_global, mask_full)
                
                x = self.final_norm(x)
                logits = self.out_head(x)
                
                loss = None
                if labels is not None:
                    loss = F.cross_entropy(
                        logits.view(-1, logits.size(-1)),
                        labels.view(-1),
                        reduction="mean"
                    )
                return logits, loss
        
        with open(args.model_config, "r") as f:
            cfg = json.load(f)
        
        # Convert dtype string to torch dtype
        dtype_map = {
            "float32": torch.float32,
            "float16": torch.float16,
            "bfloat16": torch.bfloat16,
            "float64": torch.float64,
            "torch.float32": torch.float32,
            "torch.float16": torch.float16,
            "torch.bfloat16": torch.bfloat16,
            "torch.float64": torch.float64
        }
        
        if "dtype" in cfg:
            dtype_str = cfg["dtype"]
            if isinstance(dtype_str, str):
                cfg["dtype"] = dtype_map.get(dtype_str, torch.float32)
                print(f"[INFO] Converted dtype '{dtype_str}' to {cfg['dtype']}")
            else:
                print(f"[INFO] dtype already in correct format: {cfg['dtype']}")
        
        if "qk_norm" in cfg:
            cfg["qk_norm"] = bool(cfg["qk_norm"])
        
        print("[INFO] Loading tokenizer...")
        tokenizer = Tokenizer.from_file(args.tokenizer_json)
        print("[INFO] Tokenizer loaded successfully")
        
        print(f"[INFO] Loading model to {device}...")
        model = Gemma3Model(cfg).to(device)
        print("[INFO] Gemma3Model architecture loaded")
        
        print("[INFO] Loading model weights...")
        state_dict = torch.load(args.model_pt, map_location=device)
        model.load_state_dict(state_dict, strict=True)
        model.eval()
        print("[INFO] Model weights loaded and set to eval mode")
        
        print("[INFO] Creating Gemma3LM wrapper...")
        gemma_lm = Gemma3LM(model, tokenizer, device=device)
        
        print("[INFO] Serializing with cloudpickle...")
        with open(args.gemma_lm, "wb") as f:
            cloudpickle.dump(gemma_lm, f)
        
        print("[OK] Gemma3LM object created and saved successfully")
    args:
      - --tokenizer_json
      - { inputPath: tokenizer_json }
      - --model_pt
      - { inputPath: model_pt }
      - --model_config
      - { inputPath: model_config }
      - --gemma_lm
      - { outputPath: gemma_lm }
