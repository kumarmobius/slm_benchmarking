name: BenchmarksEvals v1
description: |
  Convert a Gemma3 / GPT-NeoX checkpoint + tokenizer/config into a Hugging Face model folder.
inputs:
  - name: converted_model
    type: Model
  - name: hf_token
    type: String
    default: ""
  - name: tasks
    type: String
    default: "gpqa,mmlu_pro,bbh,math,ifeval"
outputs:
  - name: compact_metrics
    type: Data
  - name: full_results
    type: Data
  - name: emissions_csv
    type: Data
  - name: logs
    type: Data
  - name: schema_json
    type: String

implementation:
  container:
    image: kumar2004/latest:v4
    command:
      - bash
      - -lc
      - |

        cat > /tmp/hf_eval.py <<'PY'
        import argparse
        import os
        import json
        import time
        import traceback
        import numpy as np
        import torch
        import psutil
        import shutil
        from huggingface_hub import HfFolder
        from datasets import load_dataset
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from lm_eval import evaluator
        from lm_eval.tasks import TaskManager
        from codecarbon import EmissionsTracker

        def log(msg):
            ts = time.strftime("%Y-%m-%d %H:%M:%S")
            print(f"[EVAL {ts}] {msg}", flush=True)

        log("Starting hf_eval script")

        parser = argparse.ArgumentParser(description="Evaluate HF model on multiple lm_eval tasks")
        parser.add_argument("--converted_model", required=True)
        parser.add_argument("--hf_token", default="")
        parser.add_argument("--tasks", default="gpqa,mmlu_pro,bbh,math,ifeval")
        parser.add_argument("--out_compact", default="/tmp/outputs/benchmark_compact.json")
        parser.add_argument("--out_full", default="/tmp/outputs/benchmark_full.json")
        parser.add_argument("--out_emissions", default="/tmp/outputs/emissions.csv")
        parser.add_argument("--out_schema", default="/tmp/outputs/schema.json")
        args = parser.parse_args()

        log(f"Parsed args: converted_model={args.converted_model}, tasks={args.tasks}")
        CONVERTED = os.path.abspath(args.converted_model)
        OUT_COMPACT = os.path.abspath(args.out_compact)
        OUT_FULL = os.path.abspath(args.out_full)
        OUT_EMISSIONS = os.path.abspath(args.out_emissions)
        OUT_SCHEMA = os.path.abspath(args.out_schema)

        # Ensure parent dirs exist
        for p in [os.path.dirname(OUT_COMPACT), os.path.dirname(OUT_FULL), os.path.dirname(OUT_EMISSIONS), os.path.dirname(OUT_SCHEMA)]:
            if p:
                os.makedirs(p, exist_ok=True)
                log(f"Ensured directory exists: {p}")

        TOKEN = args.hf_token or ""
        if TOKEN:
            os.environ["HUGGINGFACE_HUB_TOKEN"] = TOKEN
            os.environ["HF_TOKEN"] = TOKEN
            HfFolder.save_token(TOKEN)
            log("Saved Hugging Face token for gated dataset access")

        os.environ.update({
            "HF_HUB_DISABLE_TELEMETRY": "1",
            # local cache locations internal to the container
            "HF_HOME": "/tmp/hf_cache",
            "HF_DATASETS_CACHE": "/tmp/hf_cache",
            "HUGGINGFACE_HUB_CACHE": "/tmp/hf_cache"
        })

        # Determine TASKS. Use TaskManager for math autodetection if requested.
        tm = None
        try:
            tm = TaskManager()
            avail = set(tm.task_index)
            math_tasks = sorted([t for t in avail if "math" in t.lower() or "hendrycks_math" in t.lower()])
            log(f"Available math tasks: {math_tasks}")
        except Exception as _e:
            log(f"Failed to build TaskManager for math autodetection: {_e}")
            math_tasks = []

        req_tasks = args.tasks.strip()
        if req_tasks.lower() in ("math", "math_auto"):
            TASKS = [t for t in math_tasks if "hendrycks_math" in t.lower()] or ["hendrycks_math"]
            log(f"Selected math TASKS: {TASKS}")
        else:
            TASKS = [t.strip() for t in req_tasks.split(",") if t.strip()]

        COMPOSITE_GROUP = ["mmlu_pro", "bbh", "math", "ifeval"]

        PRELOAD_MAP = {
            "gpqa": [("Idavidrein/gpqa", "gpqa_main")],
        }

        def _preload_for_task(task):
            task_key = task.lower()
            entries = PRELOAD_MAP.get(task_key)
            if not entries:
                return False
            ok_any = False
            for repo, split in entries:
                try:
                    log(f"Preloading dataset {repo} split={split} (cache_dir=/tmp/hf_cache)")
                    try:
                        load_dataset(repo, split, cache_dir="/tmp/hf_cache", token=TOKEN)
                    except TypeError:
                        load_dataset(repo, split, cache_dir="/tmp/hf_cache", use_auth_token=TOKEN)
                    log(f"Preloaded {repo}::{split}")
                    ok_any = True
                except Exception as e:
                    log(f"Preload failed for {repo}::{split}: {e}")
            return ok_any

        # Log system RAM and disk before heavy operations
        try:
            mem = psutil.virtual_memory()
            log(f" Total RAM: {mem.total / 1e9:.2f} GB")
            log(f"Available RAM: {mem.available / 1e9:.2f} GB")
        except Exception as e:
            log(f"Failed to read RAM info: {e}")

        try:
            total, used, free = shutil.disk_usage("/")
            log(f" Total storage (root): {total / 1e9:.2f} GB")
            log(f" Free storage (root): {free / 1e9:.2f} GB")
        except Exception as e:
            log(f"Failed to read disk info: {e}")

        log(f"Starting emissions tracker -> {OUT_EMISSIONS}")
        tracker = EmissionsTracker(output_dir=os.path.dirname(OUT_EMISSIONS) or ".", output_file=os.path.basename(OUT_EMISSIONS))
        tracker.start()

        log(f"Loading tokenizer + model from: {CONVERTED}")
        tokenizer = AutoTokenizer.from_pretrained(CONVERTED, trust_remote_code=False)
        _ = AutoModelForCausalLM.from_pretrained(CONVERTED, trust_remote_code=False)
        log("Tokenizer and model loaded")

        # GPU / CPU reporting
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            log(f"CUDA visible devices: {os.environ.get('CUDA_VISIBLE_DEVICES', 'all')}")
            log(f"torch reports {gpu_count} CUDA device(s).")
            for i in range(gpu_count):
                try:
                    name = torch.cuda.get_device_name(i)
                except Exception:
                    name = "<unknown>"
                try:
                    cap = torch.cuda.get_device_capability(i)
                except Exception:
                    cap = ("?", "?")
                try:
                    total_mem = torch.cuda.get_device_properties(i).total_memory / 1e9
                except Exception:
                    total_mem = None
                log(f"  GPU {i}: {name}  |  capability: {cap}  |  total_mem_gb: {total_mem}")
            eval_device = "cuda:0"
        else:
            log("CUDA not available â€” using CPU")
            eval_device = "cpu"
        log(f"Using device string for lm_eval: {eval_device}")

        def safe_json(obj):
            if isinstance(obj, dict):
                return {k: safe_json(v) for k, v in obj.items()}
            if isinstance(obj, (list, tuple)):
                return [safe_json(v) for v in obj]
            try:
                if hasattr(obj, "item"):
                    return obj.item()
            except Exception:
                pass
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            if isinstance(obj, (np.dtype, torch.dtype, torch.device, type)):
                return str(obj)
            try:
                json.dumps(obj)
                return obj
            except Exception:
                return str(obj)

        def pick_metric(d):
            for k in ("acc_norm", "acc", "accuracy"):
                if k in d and isinstance(d[k], (int, float)):
                    return k, float(d[k])
            for k, v in d.items():
                if isinstance(v, (int, float)):
                    return k, float(v)
            return None, None

        compact_results = {}
        full_results = {}

        total_tasks = len(TASKS)
        log(f"Will run {total_tasks} task(s): {TASKS}")

        for idx, task in enumerate(TASKS, start=1):
            log(f"=== Starting task {idx}/{total_tasks}: {task} ===")
            _preload_for_task(task)
            start_t = time.time()
            try:
                model_args_str = "pretrained={},tokenizer={},trust_remote_code=False".format(CONVERTED, CONVERTED)
                res = evaluator.simple_evaluate(
                    model="hf",
                    model_args=model_args_str,
                    tasks=[task],
                    batch_size="6",
                    device=eval_device,
                    num_fewshot=0,
                    limit=100,
                )
                duration = time.time() - start_t
                full_results[task] = safe_json(res)
                results_dict = res.get("results", {})
                task_result = results_dict.get(task) or (next(iter(results_dict.values())) if results_dict else None)

                if isinstance(task_result, dict):
                    metric_name, metric_value = pick_metric(task_result)
                    if metric_name is not None:
                        compact_results[task] = {metric_name: metric_value, "time_s": round(duration, 3)}
                        log(f"Task {task} succeeded: {metric_name} = {metric_value}  (time {duration:.1f}s)")
                    else:
                        compact_results[task] = {"note": "no numeric metric found", "time_s": round(duration, 3)}
                        log(f"Task {task}: no numeric metric found  (time {duration:.1f}s)")
                else:
                    compact_results[task] = {"note": "unexpected result structure", "raw": safe_json(task_result), "time_s": round(duration,3)}
                    log(f"Task {task}: unexpected result structure  (time {duration:.1f}s)")

            except Exception as e:
                duration = time.time() - start_t
                tb = traceback.format_exc()
                log(f"Task {task} failed with exception: {e}")
                print(tb, flush=True)
                compact_results[task] = {"error": str(e), "time_s": round(duration, 3)}
                full_results[task] = {"error": str(e), "traceback": tb, "time_s": round(duration, 3)}

        emissions = tracker.stop()
        # codecarbon returns kg (float or numpy); coerce to float
        try:
            emissions_val = float(emissions)
        except Exception:
            emissions_val = None

        # format CO2 string as requested
        def format_co2_str(val):
            try:
                return f"{float(val):.4f} kg"
            except Exception:
                return None

        co2_str = format_co2_str(emissions_val)
        compact_results["co2_emissions_kg"] = co2_str
        full_results["co2_emissions_kg"] = co2_str
        log(f"Emissions recorded: {co2_str}")

        # compute composite mean over COMPOSITE_GROUP if possible
        found_vals = []
        for t in COMPOSITE_GROUP:
            entry = compact_results.get(t)
            if isinstance(entry, dict):
                for k, v in entry.items():
                    if k == "time_s" or k.startswith("note") or k == "error":
                        continue
                    if isinstance(v, (int, float)):
                        found_vals.append(float(v))
                        break

        if found_vals:
            composite_score = float(np.mean(found_vals))
            compact_results["composite_mean"] = round(composite_score, 6)
            log(f"Composite mean over {COMPOSITE_GROUP}: {composite_score}")

        # combine all math task scores and compute mean
        math_scores = []
        for t_name, entry in compact_results.items():
            if "math" in t_name.lower():
                if isinstance(entry, dict):
                    for k, v in entry.items():
                        if k == "time_s" or k.startswith("note") or k == "error":
                            continue
                        if isinstance(v, (int, float)):
                            math_scores.append(float(v))
                            break
        if math_scores:
            math_mean = float(np.mean(math_scores))
            compact_results["math_mean"] = round(math_mean, 6)
            log(f"Math tasks mean across detected math tasks ({[t for t in compact_results.keys() if 'math' in t.lower()]}): {math_mean}")

        # Build schema-style JSON: each task -> rounded percent score (if available), plus co2 (string)
        schema = {}
        for task, entry in compact_results.items():
            if task in ("co2_emissions_kg", "composite_mean", "math_mean"):
                continue
            if isinstance(entry, dict):
                # find first numeric metric
                for k, v in entry.items():
                    if isinstance(v, (int, float)) and k != "time_s":
                        try:
                            # multiply by 100 to convert to percentage-style value
                            schema[task] = round(float(v) * 100.0, 2)
                        except Exception:
                            pass
                        break
        try:
            if compact_results.get("co2_emissions_kg") is not None:
                schema["co2_emissions_kg"] = compact_results["co2_emissions_kg"]
        except Exception:
            schema["co2_emissions_kg"] = compact_results.get("co2_emissions_kg")

        # Save outputs
        try:
            with open(OUT_COMPACT, "w") as f:
                json.dump(safe_json(compact_results), f, indent=2)
            log(f"Saved compact summary to: {OUT_COMPACT}")
        except Exception as e:
            log(f"Failed to save compact summary: {e}")

        try:
            with open(OUT_FULL, "w") as f:
                json.dump(safe_json(full_results), f, indent=2)
            log(f"Saved full results to: {OUT_FULL}")
        except Exception as e:
            log(f"Failed to save full results: {e}")

        try:
            with open(OUT_SCHEMA, "w") as f:
                json.dump(schema, f, indent=2)
            log(f"Saved schema JSON to: {OUT_SCHEMA}")
        except Exception as e:
            log(f"Failed to save schema JSON: {e}")

        # Print readable metrics + single-line machine JSON for easy extraction
        for task, data in compact_results.items():
            if isinstance(data, dict):
                metric_name = None
                metric_value = None
                for k, v in data.items():
                    if k == "time_s" or k.startswith("note") or k == "error":
                        continue
                    if isinstance(v, (int, float)):
                        metric_name = k
                        metric_value = v
                        break
                if metric_name is not None:
                    log(f"metric: {task} -> {metric_name}: {metric_value}")
                else:
                    log(f"metric: {task} -> no numeric metric (entry={data})")
            else:
                log(f"metric: {task} -> unexpected entry: {data}")

        try:
            metric_payload = {
                "compact": safe_json(compact_results),
                "full": safe_json(full_results),
                "co2_kg": co2_str,
                "schema": schema
            }
        except Exception:
            metric_payload = {k: round(v, 2) if isinstance(v, (int, float)) else v for k, v in results.items()}

        metric_line = "METRIC_JSON:" + json.dumps(metric_payload, separators=(",", ":"), ensure_ascii=False)
        print(metric_line, flush=True)
        import sys
        print(metric_line, file=sys.stderr, flush=True)

        log("hf_eval script completed")
        PY
        exec python3 -u /tmp/hf_eval.py "$0" "$@"
    args:
      - --converted_model
      - {inputPath: converted_model}
      - --hf_token
      - {inputValue: hf_token}
      - --tasks
      - {inputValue: tasks}
      - --out_compact
      - {outputPath: compact_metrics}
      - --out_full
      - {outputPath: full_results}
      - --out_emissions
      - {outputPath: emissions_csv}
      - --out_schema
      - {outputPath: schema_json}
