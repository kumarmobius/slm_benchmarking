name: GemmaLMEval V7
inputs:
  - name: gemma_model
    type: Data
    description: Cloud-pickled Gemma3LM object
  - name: tasks
    type: String
    default: "gpqa,mmlu_pro,bbh,math,ifeval"
    description: Comma-separated lm_eval task names (use 'math' for auto-detection)
  - name: num_fewshot
    type: Integer
    default: "0"
    description: Number of few-shot examples
  - name: limit
    type: Integer
    default: "100"
    description: Evaluation limit per task
  - name: batch_size
    type: String
    default: "auto"
    description: Batch size for evaluation
  - name: hf_token
    type: String
    default: "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"
    description: Hugging Face authentication token
outputs:
  - name: compact_metrics
    type: Data
    description: Compact JSON with task scores and CO2
  - name: full_results
    type: Data
    description: Full evaluation results with all metrics
  - name: schema_json
    type: String
    description: Clean schema with percentage scores
  - name: emissions_csv
    type: Data
    description: CodeCarbon emissions CSV
implementation:
  container:
    image: kumar2004/gemma-lm-eval:v1
    command:
      - sh
      - -c
      - |
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'immutabledict' 'psutil' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'immutabledict' 'psutil' --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import pickle
        import time
        import traceback
        import shutil
        from typing import Dict, Any, List, Optional, Tuple
        import numpy as np
        import torch
        import torch.nn as nn
        import torch.nn.functional as F
        import psutil
        from lm_eval import evaluator
        from lm_eval.tasks import TaskManager
        from codecarbon import EmissionsTracker

        def compute_rope_params(head_dim, theta_base=10_000.0, context_length=4096, dtype=torch.float32):
            assert head_dim % 2 == 0
            inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype) / head_dim))
            positions = torch.arange(context_length, dtype=dtype)
            angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)
            angles = torch.cat((angles, angles), dim=1)
            return torch.cos(angles), torch.sin(angles)

        def apply_rope(x, cos, sin):
            *_, seq_len, head_dim = x.size()
            x1, x2 = x[..., :head_dim//2], x[..., head_dim//2:]
            cos_seq = cos[:seq_len, :].to(x.dtype).unsqueeze(0).unsqueeze(0)
            sin_seq = sin[:seq_len, :].to(x.dtype).unsqueeze(0).unsqueeze(0)
            rotated = torch.cat((-x2, x1), dim=-1)
            return (x * cos_seq) + (rotated * sin_seq)

        class RMSNorm(nn.Module):
            def __init__(self, emb_dim, eps=1e-6, bias=False):
                super().__init__()
                self.eps = eps
                self.scale = nn.Parameter(torch.zeros(emb_dim))
                self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None
            
            def forward(self, x):
                orig_dtype = x.dtype
                x_f = x.float()
                var = x_f.pow(2).mean(dim=-1, keepdim=True)
                x_norm = x_f * torch.rsqrt(var + self.eps)
                out = x_norm * (1.0 + self.scale.float())
                if self.shift is not None:
                    out = out + self.shift.float()
                return out.to(orig_dtype)

        class GroupedQueryAttention(nn.Module):
            def __init__(self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, query_pre_attn_scalar=None, dtype=None):
                super().__init__()
                assert num_heads % num_kv_groups == 0
                self.num_heads = num_heads
                self.num_kv_groups = num_kv_groups
                self.group_size = num_heads // num_kv_groups
                self.head_dim = head_dim if head_dim is not None else d_in // num_heads
                self.scaling = (query_pre_attn_scalar ** -0.5) if (query_pre_attn_scalar is not None) else (self.head_dim ** -0.5)
                self.W_query = nn.Linear(d_in, num_heads * self.head_dim, bias=False, dtype=dtype)
                self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)
                self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)
                self.out_proj = nn.Linear(num_heads * self.head_dim, d_in, bias=False, dtype=dtype)
                self.q_norm = RMSNorm(self.head_dim) if qk_norm else None
                self.k_norm = RMSNorm(self.head_dim) if qk_norm else None
            
            def forward(self, x, cos, sin, mask=None):
                B, T, _ = x.shape
                q = self.W_query(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
                k = self.W_key(x).view(B, T, self.num_kv_groups, self.head_dim).transpose(1, 2)
                v = self.W_value(x).view(B, T, self.num_kv_groups, self.head_dim).transpose(1, 2)
                if self.q_norm is not None:
                    q = self.q_norm(q)
                if self.k_norm is not None:
                    k = self.k_norm(k)
                q = apply_rope(q, cos, sin)
                k = apply_rope(k, cos, sin)
                k = k.repeat_interleave(self.group_size, dim=1)
                v = v.repeat_interleave(self.group_size, dim=1)
                attn = torch.matmul(q, k.transpose(-2, -1)) * self.scaling
                if mask is not None:
                    attn = attn.masked_fill(~mask.unsqueeze(0).unsqueeze(0), float('-inf'))
                p = torch.softmax(attn, dim=-1)
                out = torch.matmul(p, v).transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)
                return self.out_proj(out)

        class FeedForward(nn.Module):
            def __init__(self, cfg):
                super().__init__()
                emb_dim = cfg["emb_dim"]
                hidden_dim = cfg["hidden_dim"]
                dtype = cfg["dtype"]
                self.fc1 = nn.Linear(emb_dim, hidden_dim, bias=False, dtype=dtype)
                self.fc2 = nn.Linear(emb_dim, hidden_dim, bias=False, dtype=dtype)
                self.fc3 = nn.Linear(hidden_dim, emb_dim, bias=False, dtype=dtype)
            
            def forward(self, x):
                return self.fc3(F.gelu(self.fc1(x)) * self.fc2(x))

        class TransformerBlock(nn.Module):
            def __init__(self, cfg, layer_type):
                super().__init__()
                self.layer_type = layer_type
                self.attn = GroupedQueryAttention(
                    d_in=cfg["emb_dim"],
                    num_heads=cfg["n_heads"],
                    num_kv_groups=cfg.get("n_kv_groups", 1),
                    head_dim=cfg["head_dim"],
                    qk_norm=cfg.get("qk_norm", False),
                    query_pre_attn_scalar=cfg.get("query_pre_attn_scalar", None),
                    dtype=cfg["dtype"]
                )
                self.input_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.post_attn_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.pre_ff_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.post_ff_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.ffn = FeedForward(cfg)
            
            def forward(self, x, cos, sin, mask=None):
                residual = x
                x = self.input_norm(x)
                x = self.post_attn_norm(self.attn(x, cos, sin, mask) + residual)
                residual = x
                x = self.pre_ff_norm(x)
                x = self.post_ff_norm(self.ffn(x) + residual)
                return x

        class Gemma3Model(nn.Module):
            def __init__(self, cfg):
                super().__init__()
                self.cfg = cfg
                vocab_size = cfg["vocab_size"]
                emb_dim = cfg["emb_dim"]
                self.token_emb = nn.Embedding(vocab_size, emb_dim, dtype=cfg["dtype"])
                self.blocks = nn.ModuleList([
                    TransformerBlock(cfg, layer_type=cfg["layer_types"][i])
                    for i in range(cfg["n_layers"])
                ])
                self.final_norm = RMSNorm(emb_dim, eps=cfg.get("rms_norm_eps", 1e-6))
                self.out_head = nn.Linear(emb_dim, vocab_size, bias=False, dtype=cfg["dtype"])
                self.cos_global, self.sin_global = compute_rope_params(
                    cfg["head_dim"], cfg["rope_base"], cfg["context_length"], torch.float32
                )
                self.cos_local, self.sin_local = compute_rope_params(
                    cfg["head_dim"], cfg["rope_local_base"], cfg["sliding_window"], torch.float32
                )
            
            def _ensure_rope_on_device(self, device):
                if self.cos_global.device != device:
                    self.cos_global = self.cos_global.to(device)
                    self.sin_global = self.sin_global.to(device)
                if self.cos_local.device != device:
                    self.cos_local = self.cos_local.to(device)
                    self.sin_local = self.sin_local.to(device)
            
            def forward(self, input_ids, labels=None):
                x = self.token_emb(input_ids).to(self.cfg["dtype"])
                B, T, _ = x.size()
                device = x.device
                self._ensure_rope_on_device(device)
                
                mask_full = torch.tril(torch.ones((T, T), dtype=torch.bool, device=device))
                window = self.cfg["sliding_window"]
                mask_sliding = torch.zeros((T, T), dtype=torch.bool, device=device)
                for i in range(T):
                    start = max(0, i - window + 1)
                    mask_sliding[i, start:i+1] = True
                
                for i, block in enumerate(self.blocks):
                    if self.cfg["layer_types"][i] == "sliding_attention":
                        x = block(x, self.cos_local, self.sin_local, mask_sliding)
                    else:
                        x = block(x, self.cos_global, self.sin_global, mask_full)
                
                x = self.final_norm(x)
                logits = self.out_head(x)
                
                loss = None
                if labels is not None:
                    loss = F.cross_entropy(
                        logits.view(-1, logits.size(-1)),
                        labels.view(-1),
                        reduction="mean"
                    )
                return logits, loss



        
        def log(msg):
            ts = time.strftime("%Y-%m-%d %H:%M:%S")
            print(f"[EVAL {ts}] {msg}", flush=True)
        
        # Composite task groups for aggregate scoring
        COMPOSITE_GROUP = ["mmlu_pro", "bbh", "math", "ifeval", "gpqa"]
        
        # Dataset preloading map (for datasets that need special handling)
        PRELOAD_MAP = {
            "gpqa": [("Idavidrein/gpqa", "gpqa_main")],
        }
        
        def safe_json(obj):
            if isinstance(obj, dict):
                return {k: safe_json(v) for k, v in obj.items()}
            if isinstance(obj, (list, tuple)):
                return [safe_json(v) for v in obj]
            try:
                if hasattr(obj, "item"):
                    return obj.item()
            except Exception:
                pass
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            if isinstance(obj, (np.dtype, torch.dtype, torch.device, type)):
                return str(obj)
            try:
                json.dumps(obj)
                return obj
            except Exception:
                return str(obj)
        
        def pick_metric(result_dict: Dict) -> Tuple[Optional[str], Optional[float]]:
            if not isinstance(result_dict, dict):
                return None, None
            
            # Priority order for metric selection
            priority_metrics = [
                "acc_norm,none", "acc_norm",
                "acc,none", "acc", "accuracy",
                "exact_match,none", "exact_match", "em",
                "f1,none", "f1",
                "bleu,none", "bleu",
                "rouge,none", "rouge",
            ]
            
            # Try priority metrics first
            for metric_name in priority_metrics:
                if metric_name in result_dict:
                    value = result_dict[metric_name]
                    if isinstance(value, (int, float)):
                        return metric_name, float(value)
            
            # Fallback: find any numeric metric
            for key, value in result_dict.items():
                if isinstance(value, (int, float)) and not key.startswith("_"):
                    return key, float(value)
            
            return None, None
        
        def detect_math_tasks(task_manager: Optional[TaskManager]) -> List[str]:
            if task_manager is None:
                return ["hendrycks_math"]
            
            try:
                available = set(task_manager.task_index)
                math_tasks = sorted([
                    t for t in available 
                    if "math" in t.lower() or "hendrycks" in t.lower()
                ])
                log(f"Detected {len(math_tasks)} math tasks: {math_tasks[:5]}{'...' if len(math_tasks) > 5 else ''}")
                
                # Prefer hendrycks_math variants
                hendrycks = [t for t in math_tasks if "hendrycks_math" in t.lower()]
                if hendrycks:
                    return hendrycks
                
                return math_tasks[:1] if math_tasks else ["hendrycks_math"]
            except Exception as e:
                log(f"Error detecting math tasks: {e}")
                return ["hendrycks_math"]
        
        def resolve_task_list(tasks_str: str, task_manager: Optional[TaskManager]) -> List[str]:
            tasks_str = tasks_str.strip()
            
            # Special case: math auto-detection
            if tasks_str.lower() in ("math", "math_auto"):
                return detect_math_tasks(task_manager)
            
            # Normal case: comma-separated list
            return [t.strip() for t in tasks_str.split(",") if t.strip()]
        
        def preload_datasets(task: str, hf_token: str) -> bool:
            task_key = task.lower()
            entries = PRELOAD_MAP.get(task_key, [])
            
            if not entries:
                return False
            
            success = False
            for repo, split in entries:
                try:
                    log(f"Preloading dataset {repo}::{split}")
                    from datasets import load_dataset
                    try:
                        load_dataset(repo, split, cache_dir="/tmp/hf_cache", token=hf_token)
                    except TypeError:
                        load_dataset(repo, split, cache_dir="/tmp/hf_cache", use_auth_token=hf_token)
                    log(f"✓ Preloaded {repo}::{split}")
                    success = True
                except Exception as e:
                    log(f"✗ Preload failed for {repo}::{split}: {e}")
            
            return success
        
        def log_system_info():
            try:
                mem = psutil.virtual_memory()
                log(f"System RAM: {mem.total / 1e9:.2f} GB total, {mem.available / 1e9:.2f} GB available")
            except Exception as e:
                log(f"Could not read RAM info: {e}")
            
            try:
                total, used, free = shutil.disk_usage("/")
                log(f"Disk space: {total / 1e9:.2f} GB total, {free / 1e9:.2f} GB free")
            except Exception as e:
                log(f"Could not read disk info: {e}")
            
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                log(f"CUDA available: {gpu_count} device(s)")
                for i in range(gpu_count):
                    try:
                        name = torch.cuda.get_device_name(i)
                        props = torch.cuda.get_device_properties(i)
                        log(f"  GPU {i}: {name} ({props.total_memory / 1e9:.2f} GB)")
                    except Exception as e:
                        log(f"  GPU {i}: <error reading properties: {e}>")
            else:
                log("CUDA not available - using CPU")
        
        def evaluate_single_task(
            model_obj,
            task: str,
            num_fewshot: int,
            limit: int,
            batch_size: str,
            hf_token: str
        ) -> Dict[str, Any]:
            result = {
                "success": False,
                "metric_name": None,
                "metric_value": None,
                "time_s": 0.0,
                "error": None,
                "traceback": None,
                "raw_results": None
            }
            
            start_time = time.time()
            
            try:
                # Preload datasets if needed
                preload_datasets(task, hf_token)
                
                # Run evaluation
                log(f"Running evaluation for task: {task}")
                res = evaluator.simple_evaluate(
                    model=model_obj,
                    tasks=[task],
                    num_fewshot=num_fewshot,
                    limit=limit,
                    batch_size=batch_size,
                )
                
                result["time_s"] = time.time() - start_time
                result["raw_results"] = safe_json(res)
                
                # Extract metric
                results_dict = res.get("results", {})
                
                # Try to find task results
                task_result = results_dict.get(task)
                if task_result is None and results_dict:
                    # Fallback: use first result
                    task_result = next(iter(results_dict.values()))
                
                if isinstance(task_result, dict):
                    metric_name, metric_value = pick_metric(task_result)
                    if metric_name is not None:
                        result["success"] = True
                        result["metric_name"] = metric_name
                        result["metric_value"] = metric_value
                        log(f"✓ Task '{task}' completed: {metric_name} = {metric_value:.6f} ({result['time_s']:.1f}s)")
                    else:
                        result["error"] = "No numeric metric found in results"
                        log(f"✗ Task '{task}': No numeric metric found")
                else:
                    result["error"] = f"Unexpected result structure: {type(task_result)}"
                    log(f"✗ Task '{task}': Unexpected result structure")
                
            except Exception as e:
                result["time_s"] = time.time() - start_time
                result["error"] = str(e)
                result["traceback"] = traceback.format_exc()
                log(f"✗ Task '{task}' failed: {e}")
                print(result["traceback"], flush=True)
            
            return result
        
        def main():
            log("=" * 70)
            log("Gemma LM Evaluation Pipeline Starting")
            log("=" * 70)
            
            # Parse arguments
            parser = argparse.ArgumentParser(description="Evaluate Gemma3LM on lm_eval tasks")
            parser.add_argument("--gemma_model", required=True, help="Path to pickled model")
            parser.add_argument("--tasks", default="gpqa,mmlu_pro,bbh,math,ifeval")
            parser.add_argument("--num_fewshot", type=int, default=0)
            parser.add_argument("--limit", type=int, default=100)
            parser.add_argument("--batch_size", default="auto")
            parser.add_argument("--hf_token", default="")
            parser.add_argument("--out_compact", required=True)
            parser.add_argument("--out_full", required=True)
            parser.add_argument("--out_schema", required=True)
            parser.add_argument("--out_emissions", required=True)
            args = parser.parse_args()
            
            log(f"Configuration:")
            log(f"  Model: {args.gemma_model}")
            log(f"  Tasks: {args.tasks}")
            log(f"  Few-shot: {args.num_fewshot}")
            log(f"  Limit: {args.limit}")
            log(f"  Batch size: {args.batch_size}")
            log(f"  HF Token: {'***' + args.hf_token[-4:] if args.hf_token else 'None'}")
            
            # Setup HF authentication
            if args.hf_token:
                os.environ["HF_TOKEN"] = args.hf_token
                os.environ["HUGGINGFACE_HUB_TOKEN"] = args.hf_token
                log("HuggingFace authentication configured")
            
            # Setup cache directories
            os.environ.update({
                "HF_HOME": "/tmp/hf_cache",
                "HF_DATASETS_CACHE": "/tmp/hf_cache",
                "HUGGINGFACE_HUB_CACHE": "/tmp/hf_cache",
                "HF_HUB_DISABLE_TELEMETRY": "1"
            })
            
            # Create output directories
            for path in [args.out_compact, args.out_full, args.out_schema, args.out_emissions]:
                os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
            
            # Log system info
            log_system_info()
            
            # Load model
            log(f"Loading model from: {args.gemma_model}")
            if not os.path.exists(args.gemma_model):
                log(f"FATAL: Model file not found: {args.gemma_model}")
                sys.exit(2)
            
            try:
                with open(args.gemma_model, "rb") as f:
                    gemma_lm = pickle.load(f)
                log(f"✓ Model loaded: {type(gemma_lm)}")
            except Exception as e:
                log(f"FATAL: Failed to load model: {e}")
                traceback.print_exc()
                sys.exit(2)
            
            # Initialize TaskManager for task resolution
            log("Initializing TaskManager...")
            try:
                task_manager = TaskManager()
                log(f"✓ TaskManager initialized with {len(task_manager.task_index)} tasks")
            except Exception as e:
                log(f"Warning: TaskManager initialization failed: {e}")
                task_manager = None
            
            # Resolve task list
            task_list = resolve_task_list(args.tasks, task_manager)
            log(f"Tasks to evaluate ({len(task_list)}): {task_list}")
            
            # Start emissions tracking
            log(f"Starting emissions tracking -> {args.out_emissions}")
            tracker = EmissionsTracker(
                output_dir=os.path.dirname(args.out_emissions) or ".",
                output_file=os.path.basename(args.out_emissions)
            )
            tracker.start()
            
            # Evaluate all tasks
            compact_results = {}
            full_results = {}
            
            log("=" * 70)
            log("Starting Task Evaluation")
            log("=" * 70)
            
            for idx, task in enumerate(task_list, 1):
                log(f"{'='*70}")
                log(f"Task {idx}/{len(task_list)}: {task}")
                log(f"{'='*70}")
                
                task_result = evaluate_single_task(
                    gemma_lm, task, args.num_fewshot, args.limit, 
                    args.batch_size, args.hf_token
                )
                
                # Store in full results
                full_results[task] = task_result
                
                # Store in compact results
                if task_result["success"]:
                    compact_results[task] = {
                        task_result["metric_name"]: task_result["metric_value"],
                        "time_s": round(task_result["time_s"], 3)
                    }
                else:
                    compact_results[task] = {
                        "error": task_result["error"],
                        "time_s": round(task_result["time_s"], 3)
                    }
            
            # Stop emissions tracking
            log("Stopping emissions tracking...")
            emissions = tracker.stop()
            
            try:
                emissions_val = float(emissions) if emissions is not None else 0.0
                co2_str = f"{emissions_val:.6f} kg"
            except Exception:
                co2_str = "0.000000 kg"
            
            compact_results["co2_emissions_kg"] = co2_str
            full_results["co2_emissions_kg"] = co2_str
            log(f"CO2 emissions: {co2_str}")
            
            # Calculate composite mean
            composite_values = []
            for task in COMPOSITE_GROUP:
                if task in compact_results and isinstance(compact_results[task], dict):
                    for k, v in compact_results[task].items():
                        if k not in ("time_s", "error") and isinstance(v, (int, float)):
                            composite_values.append(float(v))
                            break
            
            if composite_values:
                composite_mean = float(np.mean(composite_values))
                compact_results["composite_mean"] = round(composite_mean, 6)
                log(f"Composite mean ({len(composite_values)} tasks): {composite_mean:.6f}")
            
            # Calculate math mean
            math_values = []
            for task_name, entry in compact_results.items():
                if "math" in task_name.lower() and isinstance(entry, dict):
                    for k, v in entry.items():
                        if k not in ("time_s", "error") and isinstance(v, (int, float)):
                            math_values.append(float(v))
                            break
            
            if math_values:
                math_mean = float(np.mean(math_values))
                compact_results["math_mean"] = round(math_mean, 6)
                log(f"Math mean ({len(math_values)} tasks): {math_mean:.6f}")
            
            # Build schema (percentage scores)
            schema = {}
            for task, entry in compact_results.items():
                if task in ("co2_emissions_kg", "composite_mean", "math_mean"):
                    continue
                if isinstance(entry, dict):
                    for k, v in entry.items():
                        if isinstance(v, (int, float)) and k not in ("time_s", "error"):
                            # Convert to percentage (multiply by 100)
                            schema[task] = round(float(v) * 100.0, 2)
                            break
            
            schema["co2_emissions_kg"] = co2_str
            
            # Save all outputs
            log("=" * 70)
            log("Saving Results")
            log("=" * 70)
            
            try:
                with open(args.out_compact, "w") as f:
                    json.dump(safe_json(compact_results), f, indent=2)
                log(f"✓ Compact results: {args.out_compact}")
            except Exception as e:
                log(f"✗ Failed to save compact results: {e}")
            
            try:
                with open(args.out_full, "w") as f:
                    json.dump(safe_json(full_results), f, indent=2)
                log(f"✓ Full results: {args.out_full}")
            except Exception as e:
                log(f"✗ Failed to save full results: {e}")
            
            try:
                with open(args.out_schema, "w") as f:
                    json.dump(schema, f, indent=2)
                log(f"✓ Schema JSON: {args.out_schema}")
            except Exception as e:
                log(f"✗ Failed to save schema: {e}")
            
            # Print summary
            log("=" * 70)
            log("EVALUATION SUMMARY")
            log("=" * 70)
            for task, data in compact_results.items():
                if isinstance(data, dict):
                    for k, v in data.items():
                        if k not in ("time_s", "error"):
                            if isinstance(v, (int, float)):
                                log(f"  {task}: {k} = {v:.6f}")
                                break
                            else:
                                log(f"  {task}: {k} = {v}")
                                break
            
            # Print machine-readable JSON
            metric_payload = {
                "compact": safe_json(compact_results),
                "full": safe_json(full_results),
                "co2_kg": co2_str,
                "schema": schema
            }
            metric_line = "METRIC_JSON:" + json.dumps(metric_payload, separators=(",", ":"))
            print(metric_line, flush=True)
            print(metric_line, file=sys.stderr, flush=True)
            
            log("=" * 70)
            log("Evaluation Pipeline Completed Successfully")
            log("=" * 70)
        
        if __name__ == "__main__":
            main()
    args:
      - --gemma_model
      - {inputPath: gemma_model}
      - --tasks
      - {inputValue: tasks}
      - --num_fewshot
      - {inputValue: num_fewshot}
      - --limit
      - {inputValue: limit}
      - --batch_size
      - {inputValue: batch_size}
      - --hf_token
      - {inputValue: hf_token}
      - --out_compact
      - {outputPath: compact_metrics}
      - --out_full
      - {outputPath: full_results}
      - --out_schema
      - {outputPath: schema_json}
      - --out_emissions
      - {outputPath: emissions_csv}
