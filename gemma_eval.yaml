name: GemmaLMEval V9
inputs:
  - name: gemma_model
    type: Data
    description: Cloud-pickled Gemma3LM object
  - name: tasks
    type: String
    default: "gpqa,mmlu_pro,bbh,math,ifeval"
    description: "Comma-separated lm_eval task names. Use 'math' for hendrycks_math"
  - name: num_fewshot
    type: Integer
    default: "0"
    description: Number of few-shot examples
  - name: limit
    type: Integer
    default: "100"
    description: "Evaluation limit per task. Use -1 for all examples"
  - name: batch_size
    type: Integer
    default: "6"
    description: "Batch size for evaluation. Use -1 for auto batch size"
  - name: hf_token
    type: String
    default: ""
    description: Hugging Face authentication token
outputs:
  - name: compact_metrics
    type: Data
    description: Compact JSON with task scores and CO2
  - name: full_results
    type: Data
    description: Full evaluation results with all metrics
  - name: schema_json
    type: String
    description: Clean schema with percentage scores
  - name: emissions_csv
    type: Data
    description: CodeCarbon emissions CSV
implementation:
  container:
    image: kumar2004/gemma-lm-eval:v1
    command:
      - sh
      - -c
      - |
        (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'immutabledict' 'psutil' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'immutabledict' 'psutil' --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import pickle
        import time
        import traceback
        import shutil
        from typing import Dict, Any, List, Optional, Tuple
        import numpy as np
        import torch
        import torch.nn as nn
        import torch.nn.functional as F
        import psutil
        from lm_eval import evaluator
        from lm_eval.tasks import TaskManager
        from codecarbon import EmissionsTracker

        def compute_rope_params(head_dim, theta_base=10_000.0, context_length=4096, dtype=torch.float32):
            assert head_dim % 2 == 0
            inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype) / head_dim))
            positions = torch.arange(context_length, dtype=dtype)
            angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)
            angles = torch.cat((angles, angles), dim=1)
            return torch.cos(angles), torch.sin(angles)

        def apply_rope(x, cos, sin):
            *_, seq_len, head_dim = x.size()
            x1, x2 = x[..., :head_dim//2], x[..., head_dim//2:]
            cos_seq = cos[:seq_len, :].to(x.dtype).unsqueeze(0).unsqueeze(0)
            sin_seq = sin[:seq_len, :].to(x.dtype).unsqueeze(0).unsqueeze(0)
            rotated = torch.cat((-x2, x1), dim=-1)
            return (x * cos_seq) + (rotated * sin_seq)

        class RMSNorm(nn.Module):
            def __init__(self, emb_dim, eps=1e-6, bias=False):
                super().__init__()
                self.eps = eps
                self.scale = nn.Parameter(torch.zeros(emb_dim))
                self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None
            
            def forward(self, x):
                orig_dtype = x.dtype
                x_f = x.float()
                var = x_f.pow(2).mean(dim=-1, keepdim=True)
                x_norm = x_f * torch.rsqrt(var + self.eps)
                out = x_norm * (1.0 + self.scale.float())
                if self.shift is not None:
                    out = out + self.shift.float()
                return out.to(orig_dtype)

        class GroupedQueryAttention(nn.Module):
            def __init__(self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, query_pre_attn_scalar=None, dtype=None):
                super().__init__()
                assert num_heads % num_kv_groups == 0
                self.num_heads = num_heads
                self.num_kv_groups = num_kv_groups
                self.group_size = num_heads // num_kv_groups
                self.head_dim = head_dim if head_dim is not None else d_in // num_heads
                self.scaling = (query_pre_attn_scalar ** -0.5) if (query_pre_attn_scalar is not None) else (self.head_dim ** -0.5)
                self.W_query = nn.Linear(d_in, num_heads * self.head_dim, bias=False, dtype=dtype)
                self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)
                self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)
                self.out_proj = nn.Linear(num_heads * self.head_dim, d_in, bias=False, dtype=dtype)
                self.q_norm = RMSNorm(self.head_dim) if qk_norm else None
                self.k_norm = RMSNorm(self.head_dim) if qk_norm else None
            
            def forward(self, x, cos, sin, mask=None):
                B, T, _ = x.shape
                q = self.W_query(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
                k = self.W_key(x).view(B, T, self.num_kv_groups, self.head_dim).transpose(1, 2)
                v = self.W_value(x).view(B, T, self.num_kv_groups, self.head_dim).transpose(1, 2)
                if self.q_norm is not None:
                    q = self.q_norm(q)
                if self.k_norm is not None:
                    k = self.k_norm(k)
                q = apply_rope(q, cos, sin)
                k = apply_rope(k, cos, sin)
                k = k.repeat_interleave(self.group_size, dim=1)
                v = v.repeat_interleave(self.group_size, dim=1)
                attn = torch.matmul(q, k.transpose(-2, -1)) * self.scaling
                if mask is not None:
                    attn = attn.masked_fill(~mask.unsqueeze(0).unsqueeze(0), float('-inf'))
                p = torch.softmax(attn, dim=-1)
                out = torch.matmul(p, v).transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)
                return self.out_proj(out)

        class FeedForward(nn.Module):
            def __init__(self, cfg):
                super().__init__()
                emb_dim = cfg["emb_dim"]
                hidden_dim = cfg["hidden_dim"]
                dtype = cfg["dtype"]
                self.fc1 = nn.Linear(emb_dim, hidden_dim, bias=False, dtype=dtype)
                self.fc2 = nn.Linear(emb_dim, hidden_dim, bias=False, dtype=dtype)
                self.fc3 = nn.Linear(hidden_dim, emb_dim, bias=False, dtype=dtype)
            
            def forward(self, x):
                return self.fc3(F.gelu(self.fc1(x)) * self.fc2(x))

        class TransformerBlock(nn.Module):
            def __init__(self, cfg, layer_type):
                super().__init__()
                self.layer_type = layer_type
                self.attn = GroupedQueryAttention(
                    d_in=cfg["emb_dim"],
                    num_heads=cfg["n_heads"],
                    num_kv_groups=cfg.get("n_kv_groups", 1),
                    head_dim=cfg["head_dim"],
                    qk_norm=cfg.get("qk_norm", False),
                    query_pre_attn_scalar=cfg.get("query_pre_attn_scalar", None),
                    dtype=cfg["dtype"]
                )
                self.input_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.post_attn_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.pre_ff_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.post_ff_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.ffn = FeedForward(cfg)
            
            def forward(self, x, cos, sin, mask=None):
                residual = x
                x = self.input_norm(x)
                x = self.post_attn_norm(self.attn(x, cos, sin, mask) + residual)
                residual = x
                x = self.pre_ff_norm(x)
                x = self.post_ff_norm(self.ffn(x) + residual)
                return x

        class Gemma3Model(nn.Module):
            def __init__(self, cfg):
                super().__init__()
                self.cfg = cfg
                vocab_size = cfg["vocab_size"]
                emb_dim = cfg["emb_dim"]
                self.token_emb = nn.Embedding(vocab_size, emb_dim, dtype=cfg["dtype"])
                self.blocks = nn.ModuleList([
                    TransformerBlock(cfg, layer_type=cfg["layer_types"][i])
                    for i in range(cfg["n_layers"])
                ])
                self.final_norm = RMSNorm(emb_dim, eps=cfg.get("rms_norm_eps", 1e-6))
                self.out_head = nn.Linear(emb_dim, vocab_size, bias=False, dtype=cfg["dtype"])
                self.cos_global, self.sin_global = compute_rope_params(
                    cfg["head_dim"], cfg["rope_base"], cfg["context_length"], torch.float32
                )
                self.cos_local, self.sin_local = compute_rope_params(
                    cfg["head_dim"], cfg["rope_local_base"], cfg["sliding_window"], torch.float32
                )
            
            def _ensure_rope_on_device(self, device):
                if self.cos_global.device != device:
                    self.cos_global = self.cos_global.to(device)
                    self.sin_global = self.sin_global.to(device)
                if self.cos_local.device != device:
                    self.cos_local = self.cos_local.to(device)
                    self.sin_local = self.sin_local.to(device)
            
            def forward(self, input_ids, labels=None):
                x = self.token_emb(input_ids).to(self.cfg["dtype"])
                B, T, _ = x.size()
                device = x.device
                self._ensure_rope_on_device(device)
                
                mask_full = torch.tril(torch.ones((T, T), dtype=torch.bool, device=device))
                window = self.cfg["sliding_window"]
                mask_sliding = torch.zeros((T, T), dtype=torch.bool, device=device)
                for i in range(T):
                    start = max(0, i - window + 1)
                    mask_sliding[i, start:i+1] = True
                
                for i, block in enumerate(self.blocks):
                    if self.cfg["layer_types"][i] == "sliding_attention":
                        x = block(x, self.cos_local, self.sin_local, mask_sliding)
                    else:
                        x = block(x, self.cos_global, self.sin_global, mask_full)
                
                x = self.final_norm(x)
                logits = self.out_head(x)
                
                loss = None
                if labels is not None:
                    loss = F.cross_entropy(
                        logits.view(-1, logits.size(-1)),
                        labels.view(-1),
                        reduction="mean"
                    )
                return logits, loss

        
        def log(msg):
            ts = time.strftime("%Y-%m-%d %H:%M:%S")
            print(f"[EVAL {ts}] {msg}", flush=True)
        
        log("Starting GemmaLMEval V7 - Production Mode")
        
        # Composite task groups for aggregate scoring (use ORIGINAL names)
        COMPOSITE_GROUP = ["mmlu_pro", "bbh", "math", "ifeval", "gpqa"]
        
        # Dataset preloading map (for datasets that need special handling)
        PRELOAD_MAP = {
            "gpqa": [("Idavidrein/gpqa", "gpqa_main")],
        }
        
        def safe_json(obj):
            if isinstance(obj, dict):
                return {k: safe_json(v) for k, v in obj.items()}
            if isinstance(obj, (list, tuple)):
                return [safe_json(v) for v in obj]
            try:
                if hasattr(obj, "item"):
                    return obj.item()
            except Exception:
                pass
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            if isinstance(obj, (np.dtype, torch.dtype, torch.device, type)):
                return str(obj)
            try:
                json.dumps(obj)
                return obj
            except Exception:
                return str(obj)
        
        def pick_metric(result_dict: Dict) -> Tuple[Optional[str], Optional[float]]:
            if not isinstance(result_dict, dict):
                return None, None
            
            # Priority order for metric selection
            priority_metrics = [
                "acc_norm,none", "acc_norm",
                "acc,none", "acc", "accuracy",
                "exact_match,none", "exact_match", "em",
                "f1,none", "f1",
                "bleu,none", "bleu",
                "rouge,none", "rouge",
            ]
            
            # Try priority metrics first
            for metric_name in priority_metrics:
                if metric_name in result_dict:
                    value = result_dict[metric_name]
                    if isinstance(value, (int, float)):
                        return metric_name, float(value)
            
            # Fallback: find any numeric metric
            for key, value in result_dict.items():
                if isinstance(value, (int, float)) and not key.startswith("_"):
                    return key, float(value)
            
            return None, None
        
        def detect_math_tasks(task_manager: Optional[TaskManager]) -> List[str]:
            if task_manager is None:
                return ["hendrycks_math"]
            
            try:
                available = set(task_manager.task_index)
                math_tasks = sorted([
                    t for t in available 
                    if "hendrycks_math" in t.lower()
                ])
                
                if math_tasks:
                    log(f"Detected {len(math_tasks)} hendrycks_math tasks: {math_tasks[:3]}{'...' if len(math_tasks) > 3 else ''}")
                    return [math_tasks[0]]  # Use first hendrycks_math task
                
                log("No hendrycks_math tasks found, using fallback")
                return ["hendrycks_math"]
            except Exception as e:
                log(f"Error detecting math tasks: {e}")
                return ["hendrycks_math"]
        
        def resolve_task_list(tasks_str: str, task_manager: Optional[TaskManager]) -> Tuple[List[str], Dict[str, str]]:
            tasks_str = tasks_str.strip()
            task_name_map = {}
            
            # Special case: entire string is "math" or "math_auto"
            if tasks_str.lower() in ("math", "math_auto"):
                math_tasks = detect_math_tasks(task_manager)
                actual_task = math_tasks[0]
                task_name_map[actual_task] = "math"
                log(f"Expanded standalone 'math' to: {actual_task}")
                return [actual_task], task_name_map
            
            # Parse comma-separated tasks
            parsed_tasks = [t.strip() for t in tasks_str.split(",") if t.strip()]
            actual_tasks = []
            
            for original_task in parsed_tasks:
                task_lower = original_task.lower()
                
                # Handle "math" in comma-separated list
                if task_lower in ("math", "math_auto"):
                    math_tasks = detect_math_tasks(task_manager)
                    actual_task = math_tasks[0]
                    actual_tasks.append(actual_task)
                    task_name_map[actual_task] = "math"
                    log(f"Mapped '{original_task}' → '{actual_task}'")
                else:
                    # Use as-is
                    actual_tasks.append(original_task)
                    task_name_map[original_task] = original_task
                    
                    # Validate task exists (warning only)
                    if task_manager:
                        try:
                            available = set(task_manager.task_index)
                            if original_task not in available:
                                log(f"⚠ WARNING: Task '{original_task}' not found in task registry")
                        except Exception:
                            pass
            
            return actual_tasks, task_name_map
        
        def preload_datasets(task: str, hf_token: str) -> bool:
            task_key = task.lower()
            
            for keyword, entries in PRELOAD_MAP.items():
                if keyword in task_key:
                    success = False
                    for repo, split in entries:
                        try:
                            log(f"Preloading dataset {repo}::{split}")
                            from datasets import load_dataset
                            try:
                                load_dataset(repo, split, cache_dir="/tmp/hf_cache", token=hf_token)
                            except TypeError:
                                load_dataset(repo, split, cache_dir="/tmp/hf_cache", use_auth_token=hf_token)
                            log(f"✓ Preloaded {repo}::{split}")
                            success = True
                        except Exception as e:
                            log(f"⚠ Preload failed for {repo}::{split}: {e}")
                    return success
            
            return False
        
        def log_system_info():
            try:
                mem = psutil.virtual_memory()
                log(f"System RAM: {mem.total / 1e9:.2f} GB total, {mem.available / 1e9:.2f} GB available")
            except Exception as e:
                log(f"⚠ Could not read RAM info: {e}")
            
            try:
                total, used, free = shutil.disk_usage("/")
                log(f"Disk space: {total / 1e9:.2f} GB total, {free / 1e9:.2f} GB free")
            except Exception as e:
                log(f"⚠ Could not read disk info: {e}")
            
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                log(f"CUDA available: {gpu_count} device(s)")
                for i in range(gpu_count):
                    try:
                        name = torch.cuda.get_device_name(i)
                        props = torch.cuda.get_device_properties(i)
                        log(f"  GPU {i}: {name} ({props.total_memory / 1e9:.2f} GB)")
                    except Exception as e:
                        log(f"  GPU {i}: <error reading properties: {e}>")
            else:
                log("⚠ CUDA not available - using CPU")
        
        def evaluate_single_task(
            model_obj,
            actual_task: str,
            original_task: str,
            num_fewshot: int,
            limit: Optional[int],
            batch_size: str,
            hf_token: str
        ) -> Dict[str, Any]:
            result = {
                "success": False,
                "metric_name": None,
                "metric_value": None,
                "time_s": 0.0,
                "error": None,
                "traceback": None,
                "raw_results": None
            }
            
            start_time = time.time()
            
            try:
                # Preload datasets if needed
                preload_datasets(actual_task, hf_token)
                
                # Run evaluation
                log(f"Running evaluation:")
                log(f"  - Task: {actual_task}")
                log(f"  - Few-shot: {num_fewshot}")
                log(f"  - Limit: {limit if limit is not None else 'ALL'}")
                log(f"  - Batch size: {batch_size}")
                
                res = evaluator.simple_evaluate(
                    model=model_obj,
                    tasks=[actual_task],
                    num_fewshot=num_fewshot,
                    limit=limit,
                    batch_size=batch_size,
                )
                
                result["time_s"] = time.time() - start_time
                result["raw_results"] = safe_json(res)
                
                # Extract metric
                results_dict = res.get("results", {})
                
                # Try to find task results
                task_result = results_dict.get(actual_task)
                if task_result is None and results_dict:
                    # Fallback: use first result
                    task_result = next(iter(results_dict.values()))
                
                if isinstance(task_result, dict):
                    metric_name, metric_value = pick_metric(task_result)
                    if metric_name is not None:
                        result["success"] = True
                        result["metric_name"] = metric_name
                        result["metric_value"] = metric_value
                        log(f"✓ Task '{original_task}' completed: {metric_name} = {metric_value:.6f} ({result['time_s']:.1f}s)")
                    else:
                        result["error"] = "No numeric metric found in results"
                        log(f"⚠ Task '{original_task}': No numeric metric found")
                else:
                    result["error"] = f"Unexpected result structure: {type(task_result)}"
                    log(f"⚠ Task '{original_task}': Unexpected result structure")
                
            except Exception as e:
                result["time_s"] = time.time() - start_time
                result["error"] = str(e)
                result["traceback"] = traceback.format_exc()
                log(f"✗ Task '{original_task}' failed: {e}")
                print(result["traceback"], flush=True)
            
            return result
        
        def main():
            log("=" * 70)
            log("Gemma LM Evaluation Pipeline - Production Mode")
            log("=" * 70)
            
            # Parse arguments
            parser = argparse.ArgumentParser(description="Evaluate Gemma3LM on lm_eval tasks")
            parser.add_argument("--gemma_model", required=True, help="Path to pickled model")
            parser.add_argument("--tasks", default="gpqa,mmlu_pro,bbh,math,ifeval")
            parser.add_argument("--num_fewshot", type=int, default=0)
            parser.add_argument("--limit", type=int, default=100)
            parser.add_argument("--batch_size", default="auto")
            parser.add_argument("--hf_token", default="")
            parser.add_argument("--out_compact", required=True)
            parser.add_argument("--out_full", required=True)
            parser.add_argument("--out_schema", required=True)
            parser.add_argument("--out_emissions", required=True)
            args = parser.parse_args()
            
            # Handle limit: -1 means all examples
            if args.limit == -1:
                eval_limit = None
                log("Limit set to -1: will evaluate ALL examples")
            else:
                eval_limit = args.limit
                log(f"Limit set to {eval_limit} examples per task")
            
            # Handle batch_size: -1 means auto, convert int to str
            if isinstance(args.batch_size, int):
                if args.batch_size == -1:
                    eval_batch_size = "auto"
                    log("Batch size set to -1: using AUTO batch size")
                else:
                    eval_batch_size = str(args.batch_size)
                    log(f"Batch size set to {eval_batch_size}")
            else:
                eval_batch_size = str(args.batch_size)
                if eval_batch_size == "-1":
                    eval_batch_size = "auto"
                    log("Batch size set to -1: using AUTO batch size")
                else:
                    log(f"Batch size set to {eval_batch_size}")
            
            log(f"Configuration:")
            log(f"  Model: {args.gemma_model}")
            log(f"  Tasks: {args.tasks}")
            log(f"  Few-shot: {args.num_fewshot}")
            log(f"  Limit: {eval_limit if eval_limit is not None else 'ALL'}")
            log(f"  Batch size: {eval_batch_size}")
            log(f"  HF Token: {'***' + args.hf_token[-4:] if args.hf_token else 'None'}")
            
            # Setup HF authentication
            if args.hf_token:
                os.environ["HF_TOKEN"] = args.hf_token
                os.environ["HUGGINGFACE_HUB_TOKEN"] = args.hf_token
                log("✓ HuggingFace authentication configured")
            
            # Setup cache directories
            os.environ.update({
                "HF_HOME": "/tmp/hf_cache",
                "HF_DATASETS_CACHE": "/tmp/hf_cache",
                "HUGGINGFACE_HUB_CACHE": "/tmp/hf_cache",
                "HF_HUB_DISABLE_TELEMETRY": "1"
            })
            log("✓ Cache directories configured")
            
            # Create output directories
            for path in [args.out_compact, args.out_full, args.out_schema, args.out_emissions]:
                parent_dir = os.path.dirname(path)
                if parent_dir:
                    os.makedirs(parent_dir, exist_ok=True)
            
            # Log system info
            log_system_info()
            
            # Load model
            log(f"Loading model from: {args.gemma_model}")
            if not os.path.exists(args.gemma_model):
                log(f"✗ FATAL: Model file not found: {args.gemma_model}")
                sys.exit(2)
            
            try:
                with open(args.gemma_model, "rb") as f:
                    gemma_lm = pickle.load(f)
                log(f"✓ Model loaded: {type(gemma_lm)}")
            except Exception as e:
                log(f"✗ FATAL: Failed to load model: {e}")
                traceback.print_exc()
                sys.exit(2)
            
            # Initialize TaskManager for task resolution
            log("Initializing TaskManager...")
            try:
                task_manager = TaskManager()
                log(f"✓ TaskManager initialized with {len(task_manager.task_index)} tasks")
            except Exception as e:
                log(f"⚠ Warning: TaskManager initialization failed: {e}")
                task_manager = None
            
            # Resolve task list with name mapping
            actual_tasks, task_name_map = resolve_task_list(args.tasks, task_manager)
            log(f"Tasks to evaluate ({len(actual_tasks)}): {actual_tasks}")
            log(f"Task name mapping: {task_name_map}")
            
            # Start emissions tracking
            log(f"Starting emissions tracking -> {args.out_emissions}")
            tracker = EmissionsTracker(
                output_dir=os.path.dirname(args.out_emissions) or ".",
                output_file=os.path.basename(args.out_emissions)
            )
            tracker.start()
            
            # Evaluate all tasks
            compact_results = {}
            full_results = {}
            schema_results = {}  # For schema_json with original names
            
            log("=" * 70)
            log("Starting Task Evaluation")
            log("=" * 70)
            
            for idx, actual_task in enumerate(actual_tasks, 1):
                original_task = task_name_map.get(actual_task, actual_task)
                
                log(f"{'='*70}")
                log(f"Task {idx}/{len(actual_tasks)}: {original_task}")
                if actual_task != original_task:
                    log(f"  (evaluating as: {actual_task})")
                log(f"{'='*70}")
                
                task_result = evaluate_single_task(
                    gemma_lm, actual_task, original_task,
                    args.num_fewshot, eval_limit, 
                    eval_batch_size, args.hf_token
                )
                
                # Store in full results with ORIGINAL name
                full_results[original_task] = task_result
                
                # Store in compact results with ORIGINAL name
                if task_result["success"]:
                    compact_results[original_task] = {
                        task_result["metric_name"]: task_result["metric_value"],
                        "time_s": round(task_result["time_s"], 3)
                    }
                    # Store in schema with ORIGINAL name and percentage
                    schema_results[original_task] = round(task_result["metric_value"] * 100.0, 2)
                else:
                    compact_results[original_task] = {
                        "error": task_result["error"],
                        "time_s": round(task_result["time_s"], 3)
                    }
            
            # Stop emissions tracking
            log("Stopping emissions tracking...")
            try:
                emissions = tracker.stop()
                try:
                    co2_val = float(emissions) if emissions is not None else 0.0
                except Exception:
                    co2_val = 0.0
            except Exception as e:
                log(f"⚠ Failed to stop emissions tracker: {e}")
                co2_val = 0.0
            
            compact_results["co2_emissions_kg"] = round(co2_val, 6)
            full_results["co2_emissions_kg"] = round(co2_val, 6)
            schema_results["co2_emissions_kg"] = round(co2_val, 6)
            schema_results["ModelName"] = "Gemma3"
            log(f"✓ CO2 emissions: {co2_val:.6f} kg")
            
            # Calculate composite mean (use ORIGINAL names)
            composite_values = []
            for original_task in COMPOSITE_GROUP:
                if original_task in compact_results and isinstance(compact_results[original_task], dict):
                    for k, v in compact_results[original_task].items():
                        if k not in ("time_s", "error") and isinstance(v, (int, float)):
                            composite_values.append(float(v))
                            break
            
            if composite_values:
                composite_mean = float(np.mean(composite_values))
                compact_results["composite_mean"] = round(composite_mean, 6)
                schema_results["composite_mean"] = round(composite_mean * 100.0, 2)
                log(f"✓ Composite mean ({len(composite_values)} tasks): {composite_mean:.6f}")
            else:
                log(f"⚠ No valid scores found for composite mean calculation")
            
            # Calculate math mean for compact_results only (NOT for schema_json)
            math_values = []
            for original_task, entry in compact_results.items():
                if "math" in original_task.lower() and isinstance(entry, dict):
                    for k, v in entry.items():
                        if k not in ("time_s", "error") and isinstance(v, (int, float)):
                            math_values.append(float(v))
                            break
            
            if math_values:
                math_mean = float(np.mean(math_values))
                compact_results["math_mean"] = round(math_mean, 6)
                # NOTE: NOT adding math_mean to schema_results
                log(f"✓ Math mean ({len(math_values)} tasks): {math_mean:.6f} (in compact_results only)")
            
            # Save all outputs
            log("=" * 70)
            log("Saving Results")
            log("=" * 70)
            
            save_errors = []
            
            try:
                with open(args.out_compact, "w") as f:
                    json.dump(safe_json(compact_results), f, indent=2)
                log(f"✓ Compact results: {args.out_compact}")
            except Exception as e:
                err_msg = f"Failed to save compact results: {e}"
                log(f"✗ {err_msg}")
                save_errors.append(err_msg)
            
            try:
                with open(args.out_full, "w") as f:
                    json.dump(safe_json(full_results), f, indent=2)
                log(f"✓ Full results: {args.out_full}")
            except Exception as e:
                err_msg = f"Failed to save full results: {e}"
                log(f"✗ {err_msg}")
                save_errors.append(err_msg)
            
            try:
                with open(args.out_schema, "w") as f:
                    json.dump(schema_results, f, indent=2)
                log(f"✓ Schema JSON: {args.out_schema}")
            except Exception as e:
                err_msg = f"Failed to save schema: {e}"
                log(f"✗ {err_msg}")
                save_errors.append(err_msg)
            
            # Verify emissions CSV exists
            try:
                if os.path.exists(args.out_emissions):
                    log(f"✓ Emissions CSV: {args.out_emissions}")
                else:
                    log(f"⚠ Emissions CSV not found at: {args.out_emissions}")
            except Exception as e:
                log(f"⚠ Failed to check emissions CSV: {e}")
            
            # Print summary
            log("=" * 70)
            log("EVALUATION SUMMARY")
            log("=" * 70)
            
            for task, data in compact_results.items():
                if task in ("co2_emissions_kg", "composite_mean", "math_mean"):
                    continue
                if isinstance(data, dict):
                    for k, v in data.items():
                        if k not in ("time_s", "error"):
                            if isinstance(v, (int, float)):
                                log(f"  {task}: {k} = {v:.6f}")
                                break
                            else:
                                log(f"  {task}: {k} = {v}")
                                break
            
            if compact_results.get("composite_mean"):
                log(f"Composite Mean: {compact_results['composite_mean']:.6f}")
            if compact_results.get("math_mean"):
                log(f"Math Mean: {compact_results['math_mean']:.6f}")
            log(f"CO2 Emissions: {co2_val} kg")
            
            # Print machine-readable JSON
            metric_payload = {
                "compact": safe_json(compact_results),
                "full": safe_json(full_results),
                "schema": schema_results,
                "co2_kg": compact_results.get("co2_emissions_kg", 0.0),
                "save_errors": save_errors if save_errors else None
            }
            metric_line = "METRIC_JSON:" + json.dumps(metric_payload, separators=(",", ":"))
            print(metric_line, flush=True)
            print(metric_line, file=sys.stderr, flush=True)
            
            # Final status
            if save_errors:
                log(f"⚠ Completed with {len(save_errors)} save error(s)")
                for err in save_errors:
                    log(f"  - {err}")
            else:
                log("=" * 70)
                log("✓ Evaluation Pipeline Completed Successfully - Production Ready")
                log("=" * 70)
        
        if __name__ == "__main__":
            main()
    args:
      - --gemma_model
      - {inputPath: gemma_model}
      - --tasks
      - {inputValue: tasks}
      - --num_fewshot
      - {inputValue: num_fewshot}
      - --limit
      - {inputValue: limit}
      - --batch_size
      - {inputValue: batch_size}
      - --hf_token
      - {inputValue: hf_token}
      - --out_compact
      - {outputPath: compact_metrics}
      - --out_full
      - {outputPath: full_results}
      - --out_schema
      - {outputPath: schema_json}
      - --out_emissions
      - {outputPath: emissions_csv}
