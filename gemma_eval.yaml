name: GemmaLMEval V4
inputs:
  - name: gemma_model
    type: Model
    description: Cloud-pickled Gemma3LM object
  - name: tasks
    type: String
    default: "ifeval,math,musr,mmlu_pro,gpqa"
    description: Comma-separated lm_eval task names
  - name: num_fewshot
    type: Integer
    default: "0"
    description: Number of few-shot examples
  - name: limit
    type: Integer
    default: "1"
    description: Evaluation limit
  - name: hf_token
    type: String
    default: "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"
    description: Hugging Face authentication token
outputs:
  - name: eval_results
    type: String
    description: Evaluation results JSON with ifeval,math,musr,mmlu_pro,gpqa,co2 fields
implementation:
  container:
    image: kumar2004/gemma-lm-eval:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import pickle
        import traceback
        from typing import Dict, Any, List
        import torch
        import torch.nn as nn
        import torch.nn.functional as F
        from lm_eval import evaluator
        from codecarbon import EmissionsTracker
        
        # Model definitions remain exactly the same as before
        def compute_rope_params(head_dim, theta_base=10_000.0, context_length=4096, dtype=torch.float32):
            assert head_dim % 2 == 0
            inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype) / head_dim))
            positions = torch.arange(context_length, dtype=dtype)
            angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)
            angles = torch.cat((angles, angles), dim=1)
            return torch.cos(angles), torch.sin(angles)
        
        def apply_rope(x, cos, sin):
            *_, seq_len, head_dim = x.size()
            x1, x2 = x[..., :head_dim//2], x[..., head_dim//2:]
            cos_seq = cos[:seq_len, :].to(x.dtype).unsqueeze(0).unsqueeze(0)
            sin_seq = sin[:seq_len, :].to(x.dtype).unsqueeze(0).unsqueeze(0)
            rotated = torch.cat((-x2, x1), dim=-1)
            return (x * cos_seq) + (rotated * sin_seq)
        
        class RMSNorm(nn.Module):
            def __init__(self, emb_dim, eps=1e-6, bias=False):
                super().__init__()
                self.eps = eps
                self.scale = nn.Parameter(torch.zeros(emb_dim))
                self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None
            
            def forward(self, x):
                orig_dtype = x.dtype
                x_f = x.float()
                var = x_f.pow(2).mean(dim=-1, keepdim=True)
                x_norm = x_f * torch.rsqrt(var + self.eps)
                out = x_norm * (1.0 + self.scale.float())
                if self.shift is not None:
                    out = out + self.shift.float()
                return out.to(orig_dtype)
        
        class GroupedQueryAttention(nn.Module):
            def __init__(self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, query_pre_attn_scalar=None, dtype=None):
                super().__init__()
                assert num_heads % num_kv_groups == 0
                self.num_heads = num_heads
                self.num_kv_groups = num_kv_groups
                self.group_size = num_heads // num_kv_groups
                self.head_dim = head_dim if head_dim is not None else d_in // num_heads
                self.scaling = (query_pre_attn_scalar ** -0.5) if (query_pre_attn_scalar is not None) else (self.head_dim ** -0.5)
                self.W_query = nn.Linear(d_in, num_heads * self.head_dim, bias=False, dtype=dtype)
                self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)
                self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)
                self.out_proj = nn.Linear(num_heads * self.head_dim, d_in, bias=False, dtype=dtype)
                self.q_norm = RMSNorm(self.head_dim) if qk_norm else None
                self.k_norm = RMSNorm(self.head_dim) if qk_norm else None
            
            def forward(self, x, cos, sin, mask=None):
                B, T, _ = x.shape
                q = self.W_query(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
                k = self.W_key(x).view(B, T, self.num_kv_groups, self.head_dim).transpose(1, 2)
                v = self.W_value(x).view(B, T, self.num_kv_groups, self.head_dim).transpose(1, 2)
                if self.q_norm is not None:
                    q = self.q_norm(q)
                if self.k_norm is not None:
                    k = self.k_norm(k)
                q = apply_rope(q, cos, sin)
                k = apply_rope(k, cos, sin)
                k = k.repeat_interleave(self.group_size, dim=1)
                v = v.repeat_interleave(self.group_size, dim=1)
                attn = torch.matmul(q, k.transpose(-2, -1)) * self.scaling
                if mask is not None:
                    attn = attn.masked_fill(~mask.unsqueeze(0).unsqueeze(0), float('-inf'))
                p = torch.softmax(attn, dim=-1)
                out = torch.matmul(p, v).transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_dim)
                return self.out_proj(out)
        
        class FeedForward(nn.Module):
            def __init__(self, cfg):
                super().__init__()
                emb_dim = cfg["emb_dim"]
                hidden_dim = cfg["hidden_dim"]
                dtype = cfg["dtype"]
                self.fc1 = nn.Linear(emb_dim, hidden_dim, bias=False, dtype=dtype)
                self.fc2 = nn.Linear(emb_dim, hidden_dim, bias=False, dtype=dtype)
                self.fc3 = nn.Linear(hidden_dim, emb_dim, bias=False, dtype=dtype)
            
            def forward(self, x):
                return self.fc3(F.gelu(self.fc1(x)) * self.fc2(x))
        
        class TransformerBlock(nn.Module):
            def __init__(self, cfg, layer_type):
                super().__init__()
                self.layer_type = layer_type
                self.attn = GroupedQueryAttention(
                    d_in=cfg["emb_dim"],
                    num_heads=cfg["n_heads"],
                    num_kv_groups=cfg.get("n_kv_groups", 1),
                    head_dim=cfg["head_dim"],
                    qk_norm=cfg.get("qk_norm", False),
                    query_pre_attn_scalar=cfg.get("query_pre_attn_scalar", None),
                    dtype=cfg["dtype"]
                )
                self.input_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.post_attn_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.pre_ff_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.post_ff_norm = RMSNorm(cfg["emb_dim"], eps=cfg.get("rms_norm_eps", 1e-6))
                self.ffn = FeedForward(cfg)
            
            def forward(self, x, cos, sin, mask=None):
                residual = x
                x = self.input_norm(x)
                x = self.post_attn_norm(self.attn(x, cos, sin, mask) + residual)
                residual = x
                x = self.pre_ff_norm(x)
                x = self.post_ff_norm(self.ffn(x) + residual)
                return x
        
        class Gemma3Model(nn.Module):
            def __init__(self, cfg):
                super().__init__()
                self.cfg = cfg
                vocab_size = cfg["vocab_size"]
                emb_dim = cfg["emb_dim"]
                self.token_emb = nn.Embedding(vocab_size, emb_dim, dtype=cfg["dtype"])
                self.blocks = nn.ModuleList([
                    TransformerBlock(cfg, layer_type=cfg["layer_types"][i])
                    for i in range(cfg["n_layers"])
                ])
                self.final_norm = RMSNorm(emb_dim, eps=cfg.get("rms_norm_eps", 1e-6))
                self.out_head = nn.Linear(emb_dim, vocab_size, bias=False, dtype=cfg["dtype"])
                self.cos_global, self.sin_global = compute_rope_params(
                    cfg["head_dim"], cfg["rope_base"], cfg["context_length"], torch.float32
                )
                self.cos_local, self.sin_local = compute_rope_params(
                    cfg["head_dim"], cfg["rope_local_base"], cfg["sliding_window"], torch.float32
                )
            
            def _ensure_rope_on_device(self, device):
                if self.cos_global.device != device:
                    self.cos_global = self.cos_global.to(device)
                    self.sin_global = self.sin_global.to(device)
                if self.cos_local.device != device:
                    self.cos_local = self.cos_local.to(device)
                    self.sin_local = self.sin_local.to(device)
            
            def forward(self, input_ids, labels=None):
                x = self.token_emb(input_ids).to(self.cfg["dtype"])
                B, T, _ = x.size()
                device = x.device
                self._ensure_rope_on_device(device)
                
                mask_full = torch.tril(torch.ones((T, T), dtype=torch.bool, device=device))
                window = self.cfg["sliding_window"]
                mask_sliding = torch.zeros((T, T), dtype=torch.bool, device=device)
                for i in range(T):
                    start = max(0, i - window + 1)
                    mask_sliding[i, start:i+1] = True
                
                for i, block in enumerate(self.blocks):
                    if self.cfg["layer_types"][i] == "sliding_attention":
                        x = block(x, self.cos_local, self.sin_local, mask_sliding)
                    else:
                        x = block(x, self.cos_global, self.sin_global, mask_full)
                
                x = self.final_norm(x)
                logits = self.out_head(x)
                
                loss = None
                if labels is not None:
                    loss = F.cross_entropy(
                        logits.view(-1, logits.size(-1)),
                        labels.view(-1),
                        reduction="mean"
                    )
                return logits, loss

        def extract_score_from_task(task_result: Dict) -> float:
            score = None
            
            # Try common score keys
            common_keys = ["acc,none", "acc", "word_perplexity,none", "ppl,none", 
                          "f1", "bleu", "rouge", "em", "exact_match"]
            
            for key in common_keys:
                if key in task_result:
                    value = task_result[key]
                    if isinstance(value, (int, float)):
                        score = float(value)
                        break
            
            # If no common key found, try to find any numeric value
            if score is None:
                for key, value in task_result.items():
                    if isinstance(value, (int, float)):
                        score = float(value)
                        break
            
            return score

        def calculate_subtask_average(results: Dict, task_name: str) -> float:
            subtask_scores = []
            
            # Look for subtasks in the results
            for key, value in results.items():
                # Check if this is a subtask (contains task_name with suffix)
                if task_name in key and key != task_name:
                    score = extract_score_from_task(value)
                    if score is not None:
                        subtask_scores.append(score)
            
            # If we found subtasks, calculate average
            if subtask_scores:
                avg_score = sum(subtask_scores) / len(subtask_scores)
                return avg_score * 100  # Multiply by 100 as requested
            
            return None

        def main():
            parser = argparse.ArgumentParser(
                description="Run lm_eval on a cloud-pickled Gemma3LM (with runtime torch patches)"
            )

            # Inputs
            parser.add_argument(
                "--gemma_model",
                required=True,
                help="Path to cloud-pickled Gemma3LM"
            )
            parser.add_argument(
                "--tasks",
                required=True,
                help="Comma-separated lm_eval tasks"
            )
            parser.add_argument(
                "--num_fewshot",
                type=int,
                required=True,
                help="Number of few-shot examples"
            )
            parser.add_argument(
                "--limit",
                type=int,
                required=True,
                help="Evaluation limit"
            )
            parser.add_argument(
                "--hf_token",
                required=True,
                help="Hugging Face authentication token"
            )

            # Outputs
            parser.add_argument(
                "--out_results",
                required=True,
                help="Path to write evaluation results JSON"
            )

            args = parser.parse_args()

            print("[EVAL] Parsed arguments:")
            for k, v in vars(args).items():
                if k == "hf_token":
                    print(f"  {k}: {'*' * 8}{v[-4:]}" if v else f"  {k}: (empty)")
                else:
                    print(f"  {k}: {v}")

            # Set HF token for authentication
            if args.hf_token and args.hf_token.strip():
                os.environ["HF_TOKEN"] = args.hf_token.strip()
                print("[EVAL] HF token set in environment")
            
            # Verify model exists
            if not os.path.exists(args.gemma_model):
                print(f"[FATAL] gemma_model not found: {args.gemma_model}", file=sys.stderr)
                sys.exit(2)

            print("[EVAL] Loading Gemma3LM from pickle")
            try:
                with open(args.gemma_model, "rb") as f:
                    gemma_lm = pickle.load(f)
                print("[EVAL] Loaded object type:", type(gemma_lm))
            except Exception as e:
                print(f"[FATAL] Failed to load model: {e}", file=sys.stderr)
                sys.exit(2)

            # Parse task list
            task_list = [t.strip() for t in args.tasks.split(",") if t.strip()]
            if not task_list:
                print("[FATAL] No valid tasks provided", file=sys.stderr)
                sys.exit(3)
            
            print(f"[EVAL] Tasks to evaluate: {task_list}")
            
            # Initialize result schema - all as strings initially
            final_results = {
                "ifeval": None,
                "math": None,
                "musr": None,
                "mmlu_pro": None,
                "gpqa": None,
                "co2": None
            }
            
            # Initialize CodeCarbon tracker
            print("[EVAL] Starting CodeCarbon emissions tracking")
            tracker = EmissionsTracker()
            tracker.start()
            
            # Evaluate tasks sequentially with error handling
            for task in task_list:
                print(f"[EVAL] {'='*60}")
                print(f"[EVAL] Evaluating task: {task}")
                print(f"[EVAL] {'='*60}")
                
                try:
                    # Run evaluation for single task
                    results = evaluator.simple_evaluate(
                        model=gemma_lm,
                        tasks=[task],
                        num_fewshot=args.num_fewshot,
                        limit=args.limit
                    )
                    
                    # Check if we have results
                    if "results" not in results or not results["results"]:
                        print(f"[EVAL] Warning: No results for task '{task}'")
                        continue
                    
                    task_results = results["results"]
                    
                    # Handle BBH-like tasks with subtasks
                    if task == "bbh" or any(f"{task}_" in key for key in task_results.keys()):
                        # Calculate average of subtasks
                        avg_score = calculate_subtask_average(task_results, task)
                        
                        if avg_score is not None:
                            print(f"[EVAL] Task '{task}' has {len([k for k in task_results.keys() if task in k and k != task])} subtasks")
                            print(f"[EVAL] Average score (×100): {avg_score:.2f}")
                            
                            # Map to final results if task is in our schema
                            if task in final_results:
                                final_results[task] = f"{avg_score:.6f}"
                        else:
                            print(f"[EVAL] Warning: Could not calculate average for task '{task}'")
                    
                    # Handle regular tasks
                    elif task in task_results:
                        # Extract score for the main task
                        score = extract_score_from_task(task_results[task])
                        
                        if score is not None:
                            # Multiply by 100 as requested
                            final_score = score * 100
                            print(f"[EVAL] Task '{task}' completed: {score} → {final_score:.2f} (×100)")
                            
                            # Map to final results if task is in our schema
                            if task in final_results:
                                final_results[task] = f"{final_score:.6f}"
                        else:
                            print(f"[EVAL] Warning: No numeric score found for task '{task}'")
                            if task in final_results:
                                final_results[task] = "N/A"
                    
                    else:
                        print(f"[EVAL] Warning: Task '{task}' not found in results")
                        
                except Exception as e:
                    print(f"[EVAL] ERROR in task '{task}': {e}")
                    print(f"[EVAL] Traceback: {traceback.format_exc()}")
                    print(f"[EVAL] Skipping task '{task}' and continuing...")
                    
                    # Mark as N/A if task is in our schema
                    if task in final_results:
                        final_results[task] = "N/A"
                    
                    continue
            
            # Stop tracker and get CO2 emissions
            print("[EVAL] Stopping emissions tracking")
            emissions = tracker.stop()
            
            # Format CO2 emissions as string
            if emissions is not None:
                final_results["co2"] = f"{emissions:.6f} kg"
                print(f"[EVAL] Total CO2 emissions: {final_results['co2']}")
            else:
                final_results["co2"] = "0.000000 kg"
                print("[EVAL] No emissions data recorded")
            
            # Clean up final results - remove None values, convert to strings
            for key in list(final_results.keys()):
                if final_results[key] is None:
                    # Don't include tasks that weren't evaluated or didn't produce scores
                    final_results.pop(key)
                elif not isinstance(final_results[key], str):
                    # Convert any remaining non-string values to strings
                    final_results[key] = str(final_results[key])
            
            # Print final results
            print("="*60)
            print("FINAL RESULTS:")
            print("="*60)
            for key, value in final_results.items():
                print(f"{key}: {value}")
            
            # Save to file
            os.makedirs(os.path.dirname(args.out_results), exist_ok=True)
            with open(args.out_results, "w", encoding="utf-8") as f:
                json.dump(final_results, f, indent=2)
            
            print(f"[EVAL] Results written to: {args.out_results}")

        if __name__ == "__main__":
            main()
    args:
      - --gemma_model
      - {inputPath: gemma_model}
      - --tasks
      - {inputValue: tasks}
      - --num_fewshot
      - {inputValue: num_fewshot}
      - --limit
      - {inputValue: limit}
      - --hf_token
      - {inputValue: hf_token}
      - --out_results
      - {outputPath: eval_results}
